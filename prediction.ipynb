{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8858637,"sourceType":"datasetVersion","datasetId":2857519}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Root directory\ndataset_dir = \"/kaggle/input/cattle-weight-detection-model-dataset-12k/www.acmeai.tech Dataset - BMGF-LivestockWeight-CV/Pixel\"\n\ndata = []\n\n# Recursively walk through all directories\nfor root, dirs, files in os.walk(dataset_dir):\n    if os.path.basename(root).lower() == \"images\":\n        for filename in files:\n            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n                parts = filename.split('_')\n                try:\n                    if len(parts) == 5:\n                        if \"b4\" in parts[1].lower():\n                            # B4 format\n                            view = parts[2].lower()\n                            weight = float(parts[3])\n                            gender = parts[4].split('.')[0]\n                        else:\n                            # B2 format\n                            view = parts[1].lower()\n                            weight = float(parts[2])\n                            gender = parts[4].split('.')[0]\n                    elif len(parts) == 4:\n                        # B3 format\n                        view = parts[1].lower()\n                        weight = float(parts[2])\n                        gender = parts[3].split('.')[0]\n                    else:\n                        print(f\"Skipping unknown format: {filename}\")\n                        continue\n                    \n                    file_path = os.path.join(root, filename)\n                    data.append((file_path, weight, view, gender))\n                \n                except (IndexError, ValueError) as e:\n                    print(f\"Skipping invalid filename: {filename} -> {e}\")\n\n\ndf = pd.DataFrame(data, columns=['image_path', 'weight', 'view', 'gender'])\n\nprint(\"Total valid images found:\", len(df))\ndf.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FOR SEPARATE MODEL\nside_df = df[df['view'] == 's'].reset_index(drop=True)\nrear_df = df[df['view'] == 'r'].reset_index(drop=True)\n\nside_df.head()\nrear_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:58:58.862129Z","iopub.execute_input":"2025-05-16T05:58:58.862398Z","iopub.status.idle":"2025-05-16T05:58:58.876433Z","shell.execute_reply.started":"2025-05-16T05:58:58.86238Z","shell.execute_reply":"2025-05-16T05:58:58.875676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:07:27.099363Z","iopub.execute_input":"2025-05-18T18:07:27.099666Z","iopub.status.idle":"2025-05-18T18:07:27.103953Z","shell.execute_reply.started":"2025-05-18T18:07:27.099644Z","shell.execute_reply":"2025-05-18T18:07:27.103066Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FOR COMBINED MODEL**","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Create stratified split object (first split: train vs temp)\nsss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=42)\nfor train_idx, temp_idx in sss1.split(df, df['view']):\n    train_df = df.iloc[train_idx]\n    temp_df = df.iloc[temp_idx]\n\n# Now split temp into validation and test (half-half)\nsss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\nfor val_idx, test_idx in sss2.split(temp_df, temp_df['view']):\n    val_df = temp_df.iloc[val_idx]\n    test_df = temp_df.iloc[test_idx]\n\n# Check sizes\nprint(f\"Train size: {len(train_df)}\")\nprint(f\"Validation size: {len(val_df)}\")\nprint(f\"Test size: {len(test_df)}\")\n\n# Optional: Check if proportions of views are preserved\nprint(\"\\nTrain view distribution:\\n\", train_df['view'].value_counts(normalize=True))\nprint(\"\\nValidation view distribution:\\n\", val_df['view'].value_counts(normalize=True))\nprint(\"\\nTest view distribution:\\n\", test_df['view'].value_counts(normalize=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:07:30.906769Z","iopub.execute_input":"2025-05-18T18:07:30.907087Z","iopub.status.idle":"2025-05-18T18:07:30.943555Z","shell.execute_reply.started":"2025-05-18T18:07:30.907067Z","shell.execute_reply":"2025-05-18T18:07:30.942682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FOR SEPARATE MODEL**","metadata":{}},{"cell_type":"code","source":"# For side view\ntrain_side, temp_side = train_test_split(side_df, test_size=0.4, random_state=42)\nval_side, test_side = train_test_split(temp_side, test_size=0.5, random_state=42)\n\n# For front view\ntrain_rear, temp_rear = train_test_split(rear_df, test_size=0.4, random_state=42)\nval_rear, test_rear = train_test_split(temp_rear, test_size=0.5, random_state=42)\n\n# Check sizes\nprint(\"FOR SIDE\")\nprint(f\"Train size: {len(train_side)}\")\nprint(f\"Validation size: {len(val_side)}\")\nprint(f\"Test size: {len(test_side)}\")\n\nprint(\"FOR REAR\")\nprint(f\"Train size: {len(train_rear)}\")\nprint(f\"Validation size: {len(val_rear)}\")\nprint(f\"Test size: {len(test_rear)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:06.511448Z","iopub.execute_input":"2025-05-16T05:59:06.511989Z","iopub.status.idle":"2025-05-16T05:59:06.52493Z","shell.execute_reply.started":"2025-05-16T05:59:06.511965Z","shell.execute_reply":"2025-05-16T05:59:06.523828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --- 2. Data loading and preprocessing ---\n\nIMG_SIZE = (128, 128)\nBATCH_SIZE = 32\nAUTOTUNE = tf.data.AUTOTUNE\n\ndef process_path(file_path, label):\n    # Read and decode\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n\n    # Resize and normalize\n    img = tf.image.resize(img, IMG_SIZE)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    # Cast label for regression\n    label = tf.cast(label, tf.float32)\n\n    return img, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:07:34.478874Z","iopub.execute_input":"2025-05-18T18:07:34.479604Z","iopub.status.idle":"2025-05-18T18:07:34.484234Z","shell.execute_reply.started":"2025-05-18T18:07:34.47958Z","shell.execute_reply":"2025-05-18T18:07:34.483493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FOR COMBINED MODEL**","metadata":{}},{"cell_type":"code","source":"def df_to_dataset(dataframe, shuffle=True, batch_size=BATCH_SIZE):\n    file_paths = dataframe['image_path'].values\n    labels = dataframe['weight'].values.astype(np.float32)\n    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n    ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\ntrain_ds = df_to_dataset(train_df)\nval_ds = df_to_dataset(val_df, shuffle=False)\ntest_ds = df_to_dataset(test_df, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:09:26.853382Z","iopub.execute_input":"2025-05-18T18:09:26.853675Z","iopub.status.idle":"2025-05-18T18:09:26.921068Z","shell.execute_reply.started":"2025-05-18T18:09:26.853656Z","shell.execute_reply":"2025-05-18T18:09:26.920507Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FOR SEPARATE MODEL**","metadata":{}},{"cell_type":"code","source":"def df_to_dataset(dataframe, shuffle=True, batch_size=BATCH_SIZE):\n    file_paths = dataframe['image_path'].values\n    labels = dataframe['weight'].values.astype(np.float32)\n    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n    ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\ntrain_side_ds = df_to_dataset(train_side)\nval_side_ds = df_to_dataset(val_side, shuffle=False)\ntest_side_ds = df_to_dataset(test_side, shuffle=False)\n\ntrain_rear_ds = df_to_dataset(train_rear)\nval_rear_ds = df_to_dataset(val_rear, shuffle=False)\ntest_rear_ds = df_to_dataset(test_rear, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:50.683496Z","iopub.execute_input":"2025-05-16T05:59:50.684219Z","iopub.status.idle":"2025-05-16T05:59:50.800042Z","shell.execute_reply.started":"2025-05-16T05:59:50.684194Z","shell.execute_reply":"2025-05-16T05:59:50.799476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. Define a simple CNN regression model ---\n\nfrom tensorflow.keras import layers, models, Input\nimport tensorflow as tf\n\ndef residual_block(x, filters):\n    shortcut = x\n    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n    x = layers.BatchNormalization()(x)\n\n    # If shortcut doesn't match filter size, use 1x1 conv to project it\n    if shortcut.shape[-1] != filters:\n        shortcut = layers.Conv2D(filters, (1, 1), padding='same')(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    x = layers.Add()([x, shortcut])\n    x = layers.Activation('relu')(x)\n    return x\n\ndef build_model():\n    inputs = Input(shape=(*IMG_SIZE, 3))\n\n    # Initial conv layer\n    x = layers.Conv2D(32, (3, 3), padding='same')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.MaxPooling2D()(x)\n\n    # Residual Block 1\n    x = residual_block(x, 64)\n    x = layers.MaxPooling2D()(x)\n    x = layers.Dropout(0.2)(x)\n\n    # Residual Block 2\n    x = residual_block(x, 128)\n    x = layers.MaxPooling2D()(x)\n\n    # Residual Block 3\n    x = residual_block(x, 256)\n    x = layers.MaxPooling2D()(x)\n    x = layers.Dropout(0.2)(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(1)(x)  # Regression output\n\n    model = models.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(\n        optimizer='adam',\n        loss='mse',\n        metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n    )\n\n    return model\n\nmodel = build_model()\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:09:34.129692Z","iopub.execute_input":"2025-05-18T18:09:34.129964Z","iopub.status.idle":"2025-05-18T18:09:34.364696Z","shell.execute_reply.started":"2025-05-18T18:09:34.129945Z","shell.execute_reply":"2025-05-18T18:09:34.363924Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FOR COMBINED MODEL**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\n\n# Save the best model based on validation loss\ncheckpoint = ModelCheckpoint(\n    'best_model.h5',\n    monitor='val_loss',\n    verbose=1,\n    save_best_only=True,\n    mode='min'\n)\n\nearlystop_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True, verbose=1)\n\n# Train the model\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=20,\n    callbacks=[checkpoint, earlystop_cb]\n)\n\n# Plot training & validation loss\nplt.figure(figsize=(10, 5))\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss (MSE)')\nplt.title('Training and Validation Loss over Epochs')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig('Training and Validation Loss over Epochs.png')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:09:45.317434Z","iopub.execute_input":"2025-05-18T18:09:45.318155Z","iopub.status.idle":"2025-05-18T18:18:26.221587Z","shell.execute_reply.started":"2025-05-18T18:09:45.318097Z","shell.execute_reply":"2025-05-18T18:18:26.22074Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FOR SEPARATE MODEL**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\n\ndef train_and_plot(model, train_ds, val_ds, view_name):\n    # Define checkpoint path per view\n    checkpoint = ModelCheckpoint(\n        f'best_model_{view_name}.h5',\n        monitor='val_loss',\n        verbose=0,\n        save_best_only=True,\n        mode='min'\n    )\n\n    # Train the model\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=30,\n        callbacks=[checkpoint]\n    )\n\n    # Plot training & validation loss\n    plt.figure(figsize=(10, 5))\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss (MSE)')\n    plt.title(f'{view_name.capitalize()} View - Training and Validation Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(f'{view_name}_loss_plot.png')\n    plt.show()\n\n    return history\n\n# --- Train model for side view ---\nmodel_side = build_model()\nhistory_side = train_and_plot(model_side, train_side_ds, val_side_ds, 'side')\n\n# --- Train model for rear view ---\nmodel_rear = build_model()\nhistory_rear = train_and_plot(model_rear, train_rear_ds, val_rear_ds, 'rear')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T06:03:05.065379Z","iopub.execute_input":"2025-05-16T06:03:05.066311Z","iopub.status.idle":"2025-05-16T06:31:57.706354Z","shell.execute_reply.started":"2025-05-16T06:03:05.066285Z","shell.execute_reply":"2025-05-16T06:31:57.7056Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FOR COMBINED MODEL**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.metrics import RootMeanSquaredError\n\nbest_model = load_model(\n    'best_model.h5',\n    compile=False  # <- avoid automatic compile to skip error\n)\n\n# Compile again manually\nbest_model.compile(\n    optimizer='adam',\n    loss=MeanSquaredError(),\n    metrics=[RootMeanSquaredError(name='rmse')]\n)\n\n\nresults = []\ni = 0\n\nfor batch_images, batch_labels in test_ds:\n    preds = best_model.predict(batch_images).flatten()\n    actuals = batch_labels.numpy().flatten()\n    \n    for j in range(len(preds)):\n        results.append({\n            'filename': test_df.iloc[i]['image_path'],\n            'actual_weight': actuals[j],\n            'predicted_weight': preds[j]\n        })\n        i += 1\n\n# Save results to CSV\nresults_df = pd.DataFrame(results)\nresults_df.to_csv('test_predictions.csv', index=False)\n\nprint(\"âœ… Test predictions saved to 'test_predictions.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:18:46.250491Z","iopub.execute_input":"2025-05-18T18:18:46.250768Z","iopub.status.idle":"2025-05-18T18:19:08.649048Z","shell.execute_reply.started":"2025-05-18T18:18:46.250749Z","shell.execute_reply":"2025-05-18T18:19:08.648259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport numpy as np\n\nrmse = mean_squared_error(results_df['actual_weight'], results_df['predicted_weight'], squared=False)\nprint(f\"Test RMSE: {rmse:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:44:56.399393Z","iopub.execute_input":"2025-05-16T05:44:56.400243Z","iopub.status.idle":"2025-05-16T05:44:56.406146Z","shell.execute_reply.started":"2025-05-16T05:44:56.400212Z","shell.execute_reply":"2025-05-16T05:44:56.405422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FOR SEPARATE MODEL**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\n\ndef evaluate_model(model_path, test_ds, test_df, view_name):\n    # Load model\n    model = load_model(model_path, compile=False)\n    model.compile(\n        optimizer='adam',\n        loss=MeanSquaredError(),\n        metrics=[RootMeanSquaredError(name='rmse')]\n    )\n\n    results = []\n    i = 0\n\n    for batch_images, batch_labels in test_ds:\n        preds = model.predict(batch_images).flatten()\n        actuals = batch_labels.numpy().flatten()\n\n        for j in range(len(preds)):\n            results.append({\n                'filename': test_df.iloc[i]['image_path'],\n                'actual_weight': actuals[j],\n                'predicted_weight': preds[j]\n            })\n            i += 1\n\n    results_df = pd.DataFrame(results)\n    results_df.to_csv(f'test_predictions_{view_name}.csv', index=False)\n\n    rmse = mean_squared_error(results_df['actual_weight'], results_df['predicted_weight'], squared=False)\n    print(f\"âœ… Test predictions saved to 'test_predictions_{view_name}.csv'\")\n    print(f\"ðŸ“Š {view_name.capitalize()} View - Test RMSE: {rmse:.4f}\\n\")\n\n    return results_df, rmse\n\n\n# --- Evaluate for Side View ---\nresults_side_df, rmse_side = evaluate_model(\n    'best_model_side.h5', test_side_ds, test_side, 'side'\n)\n\n# --- Evaluate for Rear View ---\nresults_rear_df, rmse_rear = evaluate_model(\n    'best_model_rear.h5', test_rear_ds, test_rear, 'rear'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T06:33:30.328629Z","iopub.execute_input":"2025-05-16T06:33:30.328935Z","iopub.status.idle":"2025-05-16T06:33:48.579855Z","shell.execute_reply.started":"2025-05-16T06:33:30.328911Z","shell.execute_reply":"2025-05-16T06:33:48.578887Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FOR COMBINED MODEL**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,6))\nplt.scatter(results_df['actual_weight'], results_df['predicted_weight'], alpha=0.5)\nplt.xlabel('Actual Weight')\nplt.ylabel('Predicted Weight')\nplt.title('Actual vs Predicted Weight')\nplt.grid(True)\nplt.plot([results_df['actual_weight'].min(), results_df['actual_weight'].max()],\n         [results_df['actual_weight'].min(), results_df['actual_weight'].max()],\n         'r--')  # Ideal line\n\nplt.savefig('Actual vs Predicted Weight.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:45:03.149495Z","iopub.execute_input":"2025-05-16T05:45:03.150163Z","iopub.status.idle":"2025-05-16T05:45:03.474786Z","shell.execute_reply.started":"2025-05-16T05:45:03.15014Z","shell.execute_reply":"2025-05-16T05:45:03.474055Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FOR SEPARATE MODEL**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_actual_vs_predicted(results_df, view_name):\n    plt.figure(figsize=(8,6))\n    plt.scatter(results_df['actual_weight'], results_df['predicted_weight'], alpha=0.5)\n    plt.xlabel('Actual Weight')\n    plt.ylabel('Predicted Weight')\n    plt.title(f'{view_name.capitalize()} View: Actual vs Predicted Weight')\n    plt.grid(True)\n\n    min_val = min(results_df['actual_weight'].min(), results_df['predicted_weight'].min())\n    max_val = max(results_df['actual_weight'].max(), results_df['predicted_weight'].max())\n\n    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal Prediction')\n    plt.legend()\n\n    plt.tight_layout()\n    filename = f'Actual_vs_Predicted_Weight_{view_name}.png'\n    plt.savefig(filename)\n    plt.show()\n    print(f\"ðŸ“ˆ Plot saved to {filename}\\n\")\n\n# --- Plot for Side View ---\nplot_actual_vs_predicted(results_side_df, 'side')\n\n# --- Plot for Rear View ---\nplot_actual_vs_predicted(results_rear_df, 'rear')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T06:33:57.861552Z","iopub.execute_input":"2025-05-16T06:33:57.862392Z","iopub.status.idle":"2025-05-16T06:33:58.603647Z","shell.execute_reply.started":"2025-05-16T06:33:57.862366Z","shell.execute_reply":"2025-05-16T06:33:58.602971Z"}},"outputs":[],"execution_count":null}]}